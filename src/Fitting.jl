struct FittingLoss{F, T <: NamedTuple}
    f :: F
    sys :: System
    labels :: Vector{Symbol}
    hp :: T
end

"""
    with_hyperparams(loss::FittingLoss, hp::NamedTuple)

Given a loss function generated by [`make_loss_fn`](@ref), returns a new loss
function with individual hyperparameters overridden by the elements of `hp`.
"""
function with_hyperparams(loss::FittingLoss, hp::NamedTuple)
    hp = Base.merge(loss.hp, hp) # If keys overlapping, 2nd arg wins
    return FittingLoss(loss.f, loss.sys, loss.labels, hp)
end

"""
    make_loss_fn(f, sys, labels, hp=(;))

Returns a `loss` function to measure goodness of fit for the parameter `labels`.
This loss will commonly be used for model optimization.

Evaluation of `loss(values)` is roughly equivalent to,

```julia
sys2 = clone_system(sys)
set_params!(sys2, labels, values)
f(sys2)
```

Note that the system provided to the callback `f` will be updated using
[`set_params!`](@ref). If `f` generates an instability error (e.g., a linear
spin wave calculation at a non energy minimizing state), then `loss` will return
`Inf`.

```julia
# Construct the loss function
loss = make_loss_fn(sys, labels) do sys
    swt = SpinWaveTheory(sys; measure)
    res = intensities_bands(swt, path)
    return squared_error_bands(res, reference_energies)
end

# Measure goodness of fit for the parameter values
loss(values)

# Fit parameter values to about 6 digits of accuracy
import Optim
opts = Optim.Options(g_tol=1e-6/energy_unit, show_trace=true)
fit = Optim.optimize(loss, values, Optim.NelderMead(), opts)
```

Hyperparameters `hp`, if provided, will be forwarded as a second argument to
`f`. Use [`with_hyperparams`](@ref) to generate a new loss function with
modified `hp`. This can be useful, e.g., to support annealing over a smoothing
parameter.

```julia
loss0 = make_loss_fn(sys, labels, (; Ïµ=0.0)) do sys, hp
    ... # Loss function involving hp.Ïµ
end

for Ïµ in (1, 0.5, 0.25, 0.1)
    loss = with_hyperparams(loss0, (; Ïµ))
    fit = Optim.optimize(loss, values, method, opts)
    values = fit.minimizer
end
```

Automatic differentiation of the loss function is supported in certain special
cases (currently the [`SCGA`](@ref) calculator). This can improve efficiency and
accuracy. See [Tutorial 10](@ref "10. Fitting to diffuse scattering data") for a
concrete example.
"""
function make_loss_fn(f, sys, labels, hp=(;))
    return FittingLoss(f, sys, labels, hp)
end

function (fl::FittingLoss)(vals)
    (; f, sys, labels, hp) = fl

    sys = clone_system(sys)
    set_params!(sys, labels, vals)
    sys.active_labels = labels
    try
        if applicable(f, sys, hp)
            return f(sys, hp)
        elseif applicable(f, sys)
            return f(sys)
        else
            error("Loss function not callable")
        end
    catch err
        (err isa InstabilityError) ? Inf : rethrow(err)
    end
end

function CRC.rrule(rc::CRC.RuleConfig, fl::FittingLoss, vals)
    (; f, sys, labels, hp) = fl

    sys = clone_system(sys)
    set_params!(sys, labels, vals)
    sys.active_labels = labels
    (y, f_pb) = try
        if applicable(f, sys, hp)
            CRC.rrule_via_ad(rc, f, sys, hp)
        elseif applicable(f, sys)
            CRC.rrule_via_ad(rc, f, sys)
        else
            error("Loss function not callable")
        end
    catch err
        (err isa InstabilityError) ? (Inf, nothing) : rethrow(err)
    end

    function pullback(Î”y)
        Î”vals = if isinf(y)
            fill!(similar(vals), NaN)
        else
            _, Î”sys = f_pb(Î”y)
            CRC.unthunk(Î”sys).vals
        end
        return (CRC.NoTangent(), Î”vals)
    end

    return y, pullback
end


# Returns weighted inner products âŸ¨x,xâŸ©, âŸ¨y,yâŸ©, âŸ¨x,yâŸ©
function squared_error_aux(x, y; weights)
    (x, y) = flatten_to_vec.((x, y))
    ty = promote_type(eltype(x), eltype(y))
    w = if isnothing(weights)
        fill(one(real(ty)), length(x))
    else
        flatten_to_vec(weights)
    end
    length(x) == length(y) == length(w) || error("Mismatched input sizes")
    all(@. real(w) >= 0 && iszero(imag(w))) || error("Weights must be non-negative")

    # This functional implementation is AD-friendly
    inds = findall(i -> !isnan(x[i]) && !isnan(y[i]), eachindex(x))
    @views begin
        xÂ² = sum(@. w[inds] * abs2(x[inds]))           # |x|Â² â‰¡ âŸ¨x,xâŸ©
        yÂ² = sum(@. w[inds] * abs2(y[inds]))           # |y|Â² â‰¡ âŸ¨y,yâŸ©
        xy = sum(@. w[inds] * conj(x[inds]) * y[inds]) # âŸ¨x,yâŸ©
    end
    return (; xÂ², yÂ², xy)
end

"""
    squared_error(x, y; weights=nothing)

Normalized sum of squared errors, ``L = (1/c) \\sum_i w_i |y_i - x_i|^2``.
Weights ``w_i`` must be nonnegative and default to one.

The normalization factor is defined symmetrically, ``c = |x|^2 + |y|^2``,
involving the weighted norm,
```math
|u|^2 = \\sum_i w_i |u_i|^2.
```

Any NaN elements (``x_i`` or ``y_i``) will be interpreted as missing data and
omitted from the sum.

See also [`squared_error_with_rescaling`](@ref).
"""
function squared_error(x, y; weights=nothing)
    (; xÂ², yÂ², xy) = squared_error_aux(x, y; weights)

    # |y - x|Â² / (|x|Â²+|y|Â²)
    return 1 - 2real(xy) / (xÂ² + yÂ²)
end

"""
    squared_error_with_rescaling(x, y; weights=nothing)

Normalized sum of squared errors, ``L = (1/c) \\min_Î± \\sum_i w_i |Î± y_i -
x_i|^2``, allowing for an arbitrary rescaling ``Î±`` of the ``y`` data. Weights
``w_i`` must be nonnegative and default to one. Returns a named tuple with
fields `(; error, rescaling)` that correspond to ``L`` and the optimal ``Î±``,
respectively.

The normalization factor,
```math
c = \\sum_i w_i |x_i|^2,
```
leads to a symmetry of ``L`` in its arguments ``(x, y)``.

Any NaN elements ``x_i`` or ``y_i`` will be interpreted as missing data and
omitted from the sum.

!!! tip "Relation to the cosine-squared loss"

    Introduce the weighted inner product,
    ```math
        âŸ¨u,vâŸ© = \\sum_i w_i u_i^* v_i,
    ```
    and its associated norm, ``|u|^2 = âŸ¨u,uâŸ©``. In this notation, ``L = |Î± y - x|^2
    / |x|^2``. The optimal ``Î±`` is obtained by setting the Wirtinger derivative to
    zero, ``âˆ‚L/âˆ‚Î±^* = 0``, with solution
    ```math
        Î± = âŸ¨y, xâŸ© / |y|Â².
    ```
    In case of complex inputs, this optimal ``Î±`` absorbs an arbitrary scale _and_
    complex phase. 

    Substitution yields the symmetric expression,
    ```math  
        L = 1 - |âŸ¨x, yâŸ©|^2 / |x|^2 |y|^2.
    ```
    It may be interpreted as ``L = 1 - \\cos(Î¸)^2``, with ``Î¸`` the geometric angle
    between ``x`` and ``y`` in data-space.
"""
function squared_error_with_rescaling(x, y; weights=nothing)
    (; xÂ², yÂ², xy) = squared_error_aux(x, y; weights)

    # |Î± y - x|Â² / |x|Â² where Î± = âŸ¨y, xâŸ© / |y|Â²
    error = 1 - abs2(xy) / (xÂ² * yÂ²)
    rescaling = conj(xy) / yÂ²
    return (; error, rescaling)
end


# Simple implementation of the Sinkhorn Gibbs algorithm for balanced optimal
# transport with entropic regularization. Should be essentially the same as
# OptimalTransport.sinkhorn.
function sinkhorn_simple(Î¼::AbstractVector, Î½::AbstractVector, C::AbstractMatrix, Ïµ::Real;
                         tol=1e-8, maxiter=10_000)
    M, N = size(C)
    length(Î¼) == M && length(Î½) == N || error("Cost matrix C size mismatch")
    all(>(0), Î¼) || error("Î¼ must be strictly positive")
    all(>(0), Î½) || error("Î½ must be strictly positive")
    isapprox(sum(Î¼), sum(Î½); rtol=1e-12, atol=1e-12) || error("sum(Î¼) and sum(Î½) must match")

    tiny = eps(Float64)
    Î¼max = max(maximum(Î¼), tiny)
    Î½max = max(maximum(Î½), tiny)

    K = @. exp(-C / Ïµ)
    u = ones(M)
    v = ones(N)
    resid1 = zeros(M)
    resid2 = zeros(N)

    KTu = zeros(N)
    Kv  = zeros(M)
    mul!(Kv, K, v)
    @. Kv = max(Kv, tiny)

    for iter in 1:maxiter
        @. u = Î¼ / Kv

        mul!(KTu, K', u)
        @. KTu = max(KTu, tiny)
        @. v = Î½ / KTu

        mul!(Kv, K, v)
        @. Kv = max(Kv, tiny)

        @. resid1 = u * Kv  - Î¼
        @. resid2 = v * KTu - Î½
        row_err = norm(resid1, Inf) / Î¼max
        col_err = norm(resid2, Inf) / Î½max

        if max(row_err, col_err) â‰¤ tol
            break
        elseif iter == maxiter
            # Too noisy; breaks quickly with small Ïµ
            # @warn "Non converged" row_err col_err
        end
    end

    Î³ = Diagonal(u) * K * Diagonal(v)
    return Î³
end

# Use balanced optimal transport to smoothly assign labeled peaks E[k] to
# theoretical modes E0[m]. The assignment matrix Î³ is used to calculate a smooth
# squared error between all peaks and their closest modes.
function bands_transport_loss(E0, X0s, E; Ïƒ, Ïµ, maxiter)
    M, K = length(E0), length(E)
    M >= K || error("$M SWT modes are insufficient to match $K labeled peaks")

    # MÃ—K, cost matrix for matching mode m to peak k
    C_bare = [((E0[m] - E[k]) / Ïƒ)^2 for m in 1:M, k in 1:K]

    # A compression function for the cost matrix used in optimal transport.
    # Derived using x = log(exp(x)) = log(âˆ‘â‚™ xâ¿/n!). The truncation below yields
    # f(x) = x + O(xâ´) at small x and f(x) ~ 3log(x) at large x.
    f(x) = log1p(x + x^2/2 + x^3/6)

    # Shift each column of the cost matrix so that its smallest value is 0. In
    # principle, the Sinkhorn algorithm is invariant to such shifts -- its
    # calculated occupations Î³ should depend only differences Î”C of column (or
    # row) elements. Note, however, that this shift becomes geometrically
    # meaningful once "compression" is performed in the next step.
    colmin = minimum(C_bare, dims=1)
    C_shift = C_bare .- colmin

    # The Sinkhorn algorithm depends on C through K = exp(-C) (for simplicity,
    # assume Ïµ is scaled into C). If any matrix element of C is large (e.g.
    # -37), the corresponding occupation Î³ would be exponentially suppressed
    # (e.g. exp(-37) â‰ˆ 1e-16, effectively 0 in Float64). To mitigate precision
    # issues, we compress the dynamical range of the cost matrix elements: C â†’
    # f(C). The function f is designed so that f(C) â‰ˆ C at small C, yet grows
    # only logarithmically at large C. It is crucial to shift each column of C
    # prior to applying f(â‹…). This maintains accuracy in Î”C for the smallest
    # elements of each column. In effect, for each peak k, C_compress[m, k]
    # respects the relative costs of the closest modes m, but is allowed to
    # underestimate costs of far away modes. For purposes of least-squares model
    # fitting, empirical tests indicate that the logarithmic growth of f(C) is
    # slow enough to tame numerics, but also fast enough to reasonably suppress
    # occupations Î³ at large C.
    C_compress = f.(C_shift)

    # MÃ—(K+1) kernel for use in optimal transport. The final column is a "sink"
    # to absorb unused modes in the case of M > K. Its numerical value is
    # arbitrary (no effect on Î³).
    C = hcat(C_compress, zeros(M))

    # Use the Sinkhorn algorithm to fractionally assign modes to peaks.
    Î¼ = ones(M)              # mass for SWT modes
    Î½ = vcat(ones(K), M - K) # mass for labeled peaks (leftover goes to sink)
    Î³ = sinkhorn_simple(Î¼, Î½, C, Ïµ; maxiter)

    # Calculate the squared error for the smooth assignments Î³. It is essential
    # to use C_bare because model fitting aims to minimize the _true_ squared
    # error.
    return dot(Î³[:, 1:K], C_bare)
end

"""
    squared_error_bands_smooth(res, Es; Ïƒ)

Like `squared_error_bands` but uses entropy-regularized optimal transport to
smoothly assign spin wave modes (`res`) to labeled peak energies (`Es`). Entropy
regularization may be useful as part of an annealing procedure to search for a
globally optimal fit.

The energy parameter `Ïƒ` can be interpreted as an uncertainty in the `Es` data
and controls the amount of smoothing. This function coincides with
[`squared_error_bands`](@ref) in the limit of vanishing `Ïƒ`.
"""
function squared_error_bands_smooth(res :: Sunny.BandIntensities,
                                    Es :: Vector{Vector{Float64}};
                                    Ïƒ, Ïµ=1.0, maxiter=1_000)
    nbands = size(res.disp, 1)
    E0s = eachcol(reshape(res.disp, nbands, :))
    X0s = eachcol(reshape(res.data, nbands, :))
    length(Es) == length(E0s) || error("Mismatch in bands vs data q-length ($(length(E0s))) â‰  $(length(Es))")
    Ïƒ > 0 || error("Energy uncertainty Ïƒ must be positive")
    Ïµ > 0 || error("Entropic regularization Ïµ must be positive")
    maxiter > 0 || error("Max iteration count must be positive")

    err = sum(bands_transport_loss.(E0s, X0s, Es; Ïƒ, Ïµ, maxiter))
    return err / norm2(Es / Ïƒ)
end

"""
    squared_error_bands(res, Es)

Squared error between the discrete band energies of an
[`intensities_bands`](@ref) calculation (`res`) and experimentally labeled
intensity peak energies (`Es`) for the same set of ``ðª``-points. Each element
`Es[i]` is itself a list of labeled intensity peaks for the `i`th ``ðª``-point.
Every labeled peak must match some spin wave band, but the converse may not be
true: Spin wave band without a labeled peak counterpart do not contribute to the
squared error.

The return value is normalized by the squared magnitude of `Es`. Specifically,
if the predicted mode are uniformly zero, then the return value is exactly 1.

Internally, this function uses the [Hungarian
algorithm](https://github.com/Gnimuc/Hungarian.jl) for optimal assignment of
modes to peaks.
"""
function squared_error_bands(res :: Sunny.BandIntensities,
                             Es :: Vector{Vector{Float64}})
    nbands = size(res.disp, 1)
    E0s = eachcol(reshape(res.disp, nbands, :))
    X0s = eachcol(reshape(res.data, nbands, :))
    length(Es) == length(E0s) || error("Mismatch in bands vs data q-length ($(length(E0s))) â‰  $(length(Es))")

    err = sum(map(E0s, X0s, Es) do E0, X0, E
        M, K = length(E0), length(E)
        M >= K || error("$M SWT modes are insufficient to match $K labeled peaks")
        C = [(E0[m] - E[k])^2 for m in 1:M, k in 1:K]
        hungarian(C)[2]
    end)

    return err / norm2(Es)
end


"""
    uncertainty_matrix(loss, x)

Returns an uncertainty matrix ``U`` that describes the slackness of the loss
function ``L`` at its minimizer ``x``. Specifically, ``U = L(x) H(x)^{-1}``
where ``H = âˆ‚^2 L / âˆ‚x âˆ‚x`` is the Hessian matrix of second derivatives.

The quantity ``(U_{ii})^{1/2}`` can often be interpreted as uncertainty of the
fitted parameter ``x_i``. Similarly, ``(n^T U n)^{1/2}`` would be uncertainty in
the normalized direction ``n`` of parameter space.

There are situations where the above uncertainty estimates deviate strongly from
the true model error. For example, if the loss function is highly constraining
about the wrong minimum (e.g., due to model mispecification), then the
uncertainty estimate may be too low. Conversely, if the loss function does not
vanish for a perfect model fit (e.g., it is not a sum of squared errors), then
the uncertainty estimate may be too high.
"""
function uncertainty_matrix(loss, x; kwargs...)
    H = FiniteDiff.finite_difference_hessian(loss, x; kwargs...)
    return loss(x) * inv(H)
end


### Autodiff

Base.:+(a::SystemTangent, b::SystemTangent) = SystemTangent(a.vals .+ b.vals)
CRC.zero_tangent(t::SystemTangent) = SystemTangent(zero(t.vals))
CRC.unthunk(t::SystemTangent) = t

function CRC.ProjectTo(sys::System)
    n = length(sys.active_labels)
    function project(Î”sys)
        Î”sys = CRC.unthunk(Î”sys)
        if Î”sys isa CRC.NoTangent || Î”sys isa CRC.AbstractZero
            return SystemTangent(zeros(n))
        elseif Î”sys isa SystemTangent
            return Î”sys
        else
            # if hasproperty(Î”sys, :vals)
            #     return SystemTangent(getproperty(Î”sys, :vals))
            # end
            error("Unsupported cotangent for System: $(typeof(Î”sys))")
        end
    end
    return project
end

# Superceded by make_loss_fn. Kept here for historical interest.
#=
function with_params(sys::System, labels::Vector{Symbol}, vals::Vector{<: Real})
    sys = clone_system(sys)
    set_params!(sys, labels, vals)
    sys.active_labels = labels
    return sys
end
function CRC.rrule(::typeof(with_params), sys::System, labels, vals)
    sys2 = with_params(sys, labels, vals)
    proj_sys = CRC.ProjectTo(sys2)
    proj_vals = CRC.ProjectTo(vals)

    function pullback(Î”sys2)
        Î”sys2 = proj_sys(CRC.unthunk(Î”sys2))
        return (CRC.NoTangent(), CRC.NoTangent(), CRC.NoTangent(), proj_vals(Î”sys2.vals))
    end

    return sys2, pullback
end
=#
